16장 연습문제

1. 상태가 없는 RNN 대비 상태가 있는 RNN의 장단점은 무엇인가요?
 : 상태가 없는 RNN은 훈련한 윈도우 크기와 같거나 작은 길이의 패턴만 감지할 수 있습니다.
반면 상태가 있는 RNN은 장기 패턴을 감지할 수 있습니다. 하지만 상태가 있는 RNN을
구현하는 것이 훨씬 어렵습니다. 특히 데이터셋을 적절히 준비하는 것이 어렵습니다. 또한
상태가 있는 RNN이 항상 더 나은 것은 아닙니다. 연속된 배치는 독립 동일 분포(IID)가 아니기
때문입니다. 경사 하강법은 비IID 데이터셋에는 잘 맞지 않습니다.

2. 왜 자동 번역에 시퀀스-투-시퀀스 RNN 대신 인코더-디코더 RNN을 사용하나요?
 : 일반적으로 문장을 한 번에 단어 하나씩 번역하면 결과가 매우 좋지 않습니다. 예를 들어
프랑스 문장 'Je vous en prie'는 'You are welcome'을 의미합니다. 하지만 이를 한 단어씩
번역하면 'I you in pray'가 됩니다. 먼저 전체 문장을 읽고 난 다음에 번역하는 것이 훨씬
낫습니다. 보통의 시퀀스-투-시퀀스 RNN은 첫 단어를 읽은 후 즉시 문장을 번역하기
시작하지만 인코더-디코더 RNN은 먼저 전체 문장을 읽고 난 다음에 번역을 합니다. 이는
다음에 말할 것이 확실하지 않을 때마다 침묵을 출력하는 시퀀스-투-시퀀스 RNN으로 생각할
수도 있습니다(생중계로 번역을 하는 통역사처럼).

3. 가변 길이 입력 시퀀스를 어떻게 다룰 수 있나요? 가변 길이 출력 시퀀스는 어떤가요?
 : 배치에 있는 모든 시퀀스 길이가 동일하도록 짧은 시퀀스에 패딩을 추가하고 RNN이 패딩
토큰을 무시하도록 마스킹을 하여 가변 길이 입력 시퀀스를 처리할 수 있습니다. 성능을 더
높이려면 크기가 비슷한 시퀀스를 모아 배치를 만드는 것이 좋습니다. 래그드 텐서는 가변
길이 시퀀스를 담을 수 있고 tf.keras에서 이를 지원할 계획이므로 가변 길이 입력 시퀀스를
다루는 것이 훨씬 간편해질 것입니다. 그다음 시퀀스 마지막 다음에 오는 토큰을 무시하도록
손실 함수를 설정해야 합니다. 비슷하게 이 모델을 사용하는 코드는 시퀀스 마지막 다음에
오는 토큰을 무시해야 합니다. 하지만 일반적으로 출력 시퀀스의 길이는 미리 알지 못합니다.
따라서 시퀀스의 끝에 EOS 토큰을 출력하도록 모델을 훈련하는 것이 한 방법입니다.

4. 빔 검색이 무엇인가요? 왜 사용해야 하나요? 이를 구현하기 위해 어떤 도구를
사용할 수 있나요?
 : 빔 검색은 훈련된 인코더-디코더 모델의 성능을 향상하기 위해 사용하는 기술입니다. 예를
들면 신경망 기계 번역 시스템이 있습니다. 이 알고리즘은 가장 가능성 있는 k개(예를 들면
최상위 3개)의 출력 시퀀스를 만들어갑니다. 디코더 스텝마다 시퀀스를 한 단어만큼 늘리고
가장 가능성 있는 k개 문장만 남깁니다. 파라미터 k를 빔 너비(beam width)라고 부릅니다.
이 값이 클수록 더 많은 CPU와 RAM이 사용됩니다. 하지만 더 정확한 시스템이 됩니다.
하나의 문장을 늘리기 위해 각 스텝에서 가장 가능성 있는 다음 단어를 탐욕적으로 선택하기보다
가능성 있는 문장 몇 개를 동시에 탐색할 수 있습니다. 또한 이 기법은 병렬화하기 쉽습니다.
텐서플로 애드온을 사용하면 빔 검색을 아주 쉽게 구현할 수 있습니다.

5. 어텐션 메커니즘이 무엇인가요? 어떤 장점이 있나요?
 : 어텐션 메커니즘은 초기의 인코더-디코더 모델에서 디코더가 입력 시퀀스에 직접 접근하기
위해 사용된 기법입니다. 이렇게 하면 더 긴 입력 시퀀스를 처리할 수 있습니다. 디코더 타임
스텝마다 현재 디코더의 상태와 인코더의 전체 출력을 정렬 모델이 처리하여 입력 타임 스텝에
대한 정렬 점수를 출력합니다. 이 점수는 입력의 어느 부분이 현재 디코더의 타임 스텝에 가장
관련되어 있는지 나타냅니다. 인코더 출력의 (정렬 점수로 가중치된) 가중치 합이 디코더로
주입되어 다음 디코더 상태와 이 타임 스텝의 출력을 만듭니다. 어텐션 메커니즘을 사용하는
주요 장점은 인코더-디코더 모델이 긴 입력 시퀀스를 잘 처리할 수 있다는 점입니다. 또 다른
장점은 정렬 점수 덕분에 모델을 디버깅하고 해석하기 용이합니다. 예를 들어 모델이 실수를
하면 입력의 어느 부분에 주의가 집중되었는지 확인할 수 있습니다. 이는 문제를 분석하는 데
도움이 됩니다. 어텐션 메커니즘은 또 멀티-헤드 어텐션 층을 사용하는 트랜스포머 구조의 핵심
입니다.

6. 트랜스포머 구조에서 가장 중요한 층이 무엇인가요? 이 층의 목적이 무엇인가요?
 : 멀티-헤드 어텐션 층입니다(원본 트랜스포머 구조는 멀티-헤드 어텐션 층 18개와 마스크드
멀티-헤드 어텐션 층 6개를 포함합니다). 트랜스포머는 BERT와 GPT-2와 같은 언어 모델의
핵심입니다. 이 층의 목적은 모델이 어떤 단어가 서로 관련되어 있는지 구별하도록 돕는 것입니다.
이런 문맥 정보를 통해 단어 표현이 향상됩니다.

7. 샘플링 소프트맥스를 사용해야 할 때는 언제인가요?
 : 샘플링 소프트맥스는 클래스가 많은 (예를 들어 클래스가 수천 개나 되는) 분류 모델을 훈련할
때 사용합니다. 정답 클래스에 대해 모델이 예측한 로짓을 기반으로 크로스 엔트로피 손실의 근사치를
계산합니다. 모든 로짓에 대해 소프트맥스를 계산하여 크로스 엔트로피 손실을 추정하는 것과 비교했을
때 훈련 속도를 상당히 높여줍니다. 훈련이 끝나면 모델은 일반적인 소프트맥스 함수를 사용합니다.
즉 모든 로짓값을 기반으로 클래스 확률을 모두 계산합니다.
