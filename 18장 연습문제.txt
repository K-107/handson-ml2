18장 연습문제

1. 강화 학습을 어떻게 정의할 수 있나요?지도 학습 이나 비지도 학습과 어떻게 다른가요?
 : 강화 학습(RL)은 머신러닝의 한 분야로, 주어진 환경에서 시간이 지남에 보상이 최대화되는
행동을 할 수 있는 에이전트를 만드는 것을 목적으로 합니다. RL과 일반 지도 학습 그리고 비지도
학습 사이에는 많은 차이점이 있습니다. 다음은 그중 일부입니다.

- 지도 학습과 비지도 학습의 목적은 데이터에 있는 패턴을 찾아 이를 사용해 예측을 만드는 것입니다.
강화 학습의 목적은 좋은 정책(알고리즘)을 찾는 것입니다.
- 지도 학습과 달리 에이전트에 올바른 정답이 명시적으로 주어지지 않습니다. 에이전트는 시행 착오를
통해 학습해야 합니다.
- 비지도 학습과 달리 보상을 통한 감독의 형태가 있습니다. 에이전트에 어떻게 작업을 수행하라고
알려주지 않지만 일을 잘했는지 또는 실패했는지 알려줍니다.
- 강화 학습 에이전트는 보상을 얻기 위해 새로운 방식을 찾는 환경의 탐험과 이미 알고 있는
보상 방법을 활용하는 것 사이에 적절한 균형을 가져야 합니다. 이와는 반대로 지도 학습과
비지도 학습 시스템은 탐험에 대해 신경 쓸 필요가 없습니다. 즉, 주어진 훈련 데이터를 주입하면 됩니다.
- 지도 학습과 비지도 학습에서 훈련 샘플은 일반적으로 독립적입니다(사실 보통 무작위로 섞여 있습니다).
강화 학습에서는 연속된 관측이 보통 독립적이지 않습니다. 에이전트가 잠시 동안 움직이지 않고 환경의
같은 경역에 머물러 있을 수 있습니다. 그러므로 연속된 관측은 매우 상호 연관되어 있습니다.
어떤 경우에는 훈련 알고리즘이 독립적인 관측을 얻을 수 있도록 재생 메모리(버퍼)를 사용합니다.

2. 이 장에서 언급하지 않은 가능한 RL 애플리케이션을 세 가지 생각해보세요. 각 애플리케이션의
환경은 무엇인가요? 에이전트는 무엇인가요? 가능한 행동은 무엇인가요? 보상은 무엇인가요?
 : - 음악 개인화 -
환경은 사용자의 개인화된 웹 라디오입니다. 에이전트는 사용자에게 다음에 어떤 노래를 재생할지
결정하는 소프트웨어입니다. 가능한 행동은 카탈로그에 있는 어떤 노래를 재생하거나(사용자가 좋아할
노래를 선택해야 합니다) 광고를 재생하는 것입니다(사용자가 관심 있어 할 광고를 선택해야 합니다).
에이전트는 사용자가 노래를 들을 때마다 작은 보상을 받고 광고를 들을 때마다 큰 보상을 받습니다.
노래나 광고를 스킵하면 음수의 보상을 받고 사용자가 더 이상 듣지 않고 떠나면 큰 음수의 보상을 받습니다.
- 마케팅 -
환경은 회사의 마케팅 부서입니다. 에이전트는 고객 프로파일과 구매 이력을 바탕으로 어떤 고객에게
홍보 메일을 보낼지 결정하는 소프트웨어입니다(각 고객에 대해 보내느냐 보내지 않느냐의 두 가지 행동이
있습니다). 에이전트는 홍보 메일의 발송 비용에 대해 음수의 보상을 받고 이 캠페인으로부터 발생된
예상 매출을 양수의 보상으로 받습니다.
- 제품 배송 -
에이전트는 배송 트럭 전체를 제어하여 창고에서 실어야 할 것과 도착할 곳, 전달할 물건 등을
결정합니다. 상품이 제시간 안에 배송되면 양수의 보상을 받고 배송이 지연되면 음수의 보상을
받습니다.

3. 할인 계수는 무엇인가요? 할인 계수를 바꾸면 최적의 정책이 바뀔 수 있나요?
 : 행동의 가치를 추정할 때 강화 학습 알고리즘은 일반적으로 이 행동으로 받을 수 있는 모든 보상을
더합니다. 이때 즉시 받을 수 있는 보상은 큰 가중치를 주고 나중에 받을 보상은 낮은 가중치를 줍니다(
먼 미래보다 가까운 장래의 영향이 더 크다고 가정합니다). 이를 모델링하기 위해 매 타임 스텝마다 할인
계수가 적용됩니다. 예를 들어 할인 계수가 0.9일 때 2번의 타임 스텝 후에 받을 100의 보상은 (0.9^2)*100
로 행동의 가치를 추정합니다. 할인 계수를 현재에 비해 미래의 가치를 얼마로 보는지의 척도로 생각할 수
있습니다. 이 값이 1에 매우 가까우면 미래의 가치는 현재와 거의 동일합니다. 만약 0에 가까우면 당장의
보상만 의미가 있을 것입니다. 당연히 이 값은 최적의 정책에 큰 영향을 미칩니다. 미래를 가치 있게 여기면
종국의 보상을 기대하고 당장의 고통을 감내하려고 하는 반면, 미래의 가치를 중요하지 않게 생각하면
미래를 위한 투자보다 찾을 수 있는 당장의 보상을 선택할 것입니다.

4. 강화 학습 에어전트의 성능은 어떻게 측정할 수 있나요?
 : 간단하게 얻은 보상을 모두 더하면 됩니다. 시뮬레이션 환경에서는 많은 에피소드를 수행하고 평균적으로
얻은 전체 보상을 확인합니다(최소, 최대, 표준 편차 등을 볼 수 있습니다).

5. 신용 할당 문제가 무엇인가요? 언제 이런 문제가 발생하나요? 어떻게 이를 감소시킬 수 있나요?
 : 신용 할당 문제는 강화 학습 에이전트가 보상을 받을 때 이 보상에 기여한 행동을 알아내기 위한
직접적인 방법이 없다는 것입니다. 일반적으로 행동과 그에 따른 보상 사이에는 많은 지연이 있습니다(
예를 들어 아타리 퐁 게임에서 에이전트가 볼을 튕기는 순간과 점수를 얻는 순간 사이에는 수십 개의
타임 스텝이 있습니다). 이 문제를 감소시키는 한 가지 방법은 가능하면 에이전트에 단기 보상을 제공하는
것입니다. 이는 보통 작업에 대한 사전 지식을 필요로 합니다. 예를 들어 체스 게임을 플레이하는
에이전트를 들려고 만들려고 한다면 게임에 이겼을 때만 보상을 주는 대신 상대 말을 잡았을 때도 보상을
줄 수 있습니다.

6. 재생 메모리를 사용하는 이유는 무엇인가요?
 : 에이전트는 종종 한동안 환경의 동일한 지역에 머물러 있을 수 있어서 이 기간 동안에 얻은 경험이
매우 비슷할 것입니다. 이는 학습 알고리즘에 일종의 편향을 일으킵니다. 이 영역에서는 정책이 튜닝되지만
이 지역을 벗어나는 순간 잘 작동하지 못할 것입니다. 이 문제를 해결하기 위해 재현 메모리를 사용할 수
있습니다. 가장 최근의 경험만을 학습에 사용하는 대신 에이전트가 최근 경험과 최근이 아닌 경험들을 담은
버퍼에 기초하여 학습하게 됩니다(아마도 이것이 밤에 우리가 꿈을 꾸는 이유일지 모릅니다. 낮의 경험을
재현하고 학습하기 위해서 아닐까요?).

7. 오프-폴리시 RL 알고리즘이 무엇인가요?
 : 오프-폴리시 RL 알고리즘은 에이전트가 최적이 아닌 정책을 따르는 동안 최적 정책의 가치(예를 들어
에이전트가 최적으로 행동한다면 각 상태에서 기대할 수 있는 할인된 보상의 합)를 학습합니다. Q-러닝이
이 알고리즘의 좋은 예입니다. 반대로 온-폴리시 알고리즘은 탐험과 활용을 포함해서 에이전트가 실제로
실행한 정책의 가치를 학습합니다.