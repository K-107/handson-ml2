14장 연습문제

1. 이미지 분류에서 완전 연결 DNN보다 CNN이 나은 점은 무엇인가요?
 : 1) 연속된 층이 부분적으로 연결되어 있고 많은 가중치를 재사용하기 때문에 CNN은 완전
연결 DNN보다 훨씬 적은 파라미터를 가집니다. 따라서 훈련 속도가 훨씬 더 빠르고 과대적합의
위험을 줄이며 더 적은 훈련 데이터를 필요로 합니다. 2) CNN이 어떤 특성을 감지할 수 있는
커널을 학습하면 이미지의 어느 위치에 있는 특성이라도 감지할 수 있습니다. 반대로 DNN은
한 위치에 있는 특성을 학습하면 특정 위치에 있는 것만 감지할 수 있습니다. 이미지는 보통
반복적인 패턴을 가지므로 CNN이 이미지 분류 같은 이미지 처리 작업에서 더 적은 수의 훈련
샘플로 DNN보다 높은 일반화 성능을 낼 수 있습니다. 3) DNN은 픽셀이 어떻게 조직되어 있는지
모릅니다. 즉, 주변의 픽셀이 비슷한지 알지 못합니다. CNN 구조는 이 정보를 내포합니다. 하위층은
일반적으로 이미지의 작은 영역에 있는 특성을 구별하고, 상위층은 저 수준 특성을 더 큰 특성으로
연결합니다. 대부분의 자연적인 이미지에서 이런 구조가 잘 맞기 때문에 DNN에 비해 CNN이 유리합니다.

2. 3x3 커널, 스트라이드 2, "same" 패딩으로 된 합성곱 층 세 개로 구성된 CNN이 있습니다.
가장 아래 층은 특성 맵 100개를 출력하고 중간 층은 200개, 가장 위의 층은 400개를 출력
합니다. 입력 이미지는 200x300 픽셀의 RGB 이미지입니다. 이 CNN의 전체 파라미터 수는
얼마일까요? 32비트 부동소수를 사용한다면 이 네트워크가 하나의 샘플을 예측하기 위해
적어도 얼마의 RAM이 필요할까요? 50개의 이미지를 미니배치로 훈련할 땐 얼마가 필요할까요?
 : 이 CNN이 얼마나 많은 파라미터가 있는지 계산해보겠습니다. 첫 번째 합성곱층은 3x3
커널을 가지고 있고 입력 채널이 세 개(빨강, 초록, 파랑)이므로 각 특성 맵은 3x3x3 크기의
가중치와 하나의 편향을 가집니다. 그래서 특성 맵마다 28개의 파라미터가 있습니다.
이 합성곱층이 100개의 특성 맵을 가지므로 전체 파라미터 수는 2,800개입니다. 두 번째
합성곱층은 3x3 커널을 가지고 있고 입력은 이전층에서 만들어진 100개의 특성 맵이 되므로
각 특성 맵마다 3x3x100=900개의 가중치와 하나의 편향을 가집니다. 이 합성곱 층은
200개의 특성 맵을 만들므로 901x200=180,200개의 가중치를 가집니다. 세 번째
합성곱층도 3x3 커널을 가지고 이전 층에서 만들어진 200개의 특성이 입력되므로 각 특성
맵은 3x3x200=1,800개의 가중치와 하나의 편향을 가집니다. 이 합성곱층은 400개의
특성 맵을 만들므로 총 1,801x400=720,400개의 파라미터가 있습니다. 이를 모두 더하면
이 CNN은 2,800+180,200+720,400=903,400개의 파라미터를 가집니다.
그럼 이 신경망이 하나의 샘플을 예측할 때 (적어도) 얼마나 많은 RAM이 필요한지 계산해
보겠습니다. 먼저 각 층의 특성 맵의 크기를 계산합니다. 스트라이드 2와 SAME 패딩을
사용하기 때문에 특성 맵의 수평, 수직 차원은 각 층에서 2배로 줄어듭니다(필요하면 반올림
합니다). 따라서 입력 채널이 200x300이므로 첫 번째 층의 특성 맵은 100x150 크기이고
두 번째 특성 맵은 50x75크기이고 세 번째 층의 특성 맵은 25x38 크기가 됩니다.
32비트는 4바이트이고 첫 번째 합성곱층은 100개의 특성 맵을 가지고 있으므로, 첫 번째
층은 4x100x150x100=6백만 바이트(약 6MB)가 필요합니다. 두 번째 층은
4x50x75x200=3백만 바이트(약 3MB)가 필요합니다. 세 번째 층은 4x25x38x400=1,520,000
바이트(약 1.5MB)가 필요합니다. 그러나 한 층의 계산이 끝나면 이전 층에서 점유되었던
메모리가 반납될 수 있기 때문에 만약 최적화가 잘되어 있다면 6+3=9백만 바이트(약 9MB)
의 RAM만 필요할 것입니다(두 번째 층이 계산될 때 첫 번째 층에서 점유했던
메모리가 아직 반납되면 안 됩니다). 하지만 CNN 파라미터에 의해 점유되는 메모리가 추가로
필요합니다. 앞서 903,400개의 파라미터가 필요하다고 계산되었고 각각 4바이트를 차지하므로
3,613,600바이트(약 3.6MB)가 필요합니다. 필요한 전체 RAM은 (적어도) 12,613,600바이트
(약 12.6MB)입니다. 마지막으로 이 CNN을 50개 이미지의 미니배치로 훈련할 때 필요한 최소한의
RAM을 계산해보겠습니다. 훈련하는 동안 텐서플로는 역전파를 사용하므로 역방향 계산이 시작될
때까지 정방향에서 계산된 모든 값을 유지해야 합니다. 그러므로 하나의 샘플일 때 모든 층에서
필요한 전체 RAM을 계산하고 여기에 50을 곱해야 합니다! 지금부터는 바이트 대신 메가 바이트로
계산하겠습니다. 하나의 샘플에 대해 세 층에서 필요한 RAM은 각기 6MB, 3MB, 1.5MB입니다.
즉 샘플마다 총 10.5MB가 필요합니다. 그러므로 50개의 샘플에 대해서는 525MB의 RAM이
필요합니다. 입력 이미지 때문에 필요한 RAM인 50x4x200x300x3=3천6백만 바이트(약 36MB)와
모델 파라미터에 의해 필요한 RAM 3.6MB를 더하고 그래디언트에 필요한 약간의 RAM(이 값은
역방향 계산에서 역전파가 층을 거슬러 진행함에 따라 점차 감소하기 때문에 무시하겠습니다)을
더합니다. 이를 모두 더하면 대략 525+36+3.6=564.6MB가 됩니다. 이 값은 최적화되었을 때 필요한
최소한의 값입니다.

3. 어떤 CNN을 훈련시킬 때 GPU에서 메모리 부족이 발생했다면, 이 문제를 해결하기 위해
시도해볼 수 있는 다섯 가지는 무엇인가요?
 : CNN을 훈련시킬 때 GPU에서 메모리 부족이 발생한다면 이 문제를 해결하기 위해 (더 많은
RAM을 가진 GPU를 구매하는 것을 제외하고) 시도해볼 수 있는 다섯 가지는 다음과 같습니다.
- 미니배치의 크기를 줄입니다.
- 하나 이상의 층에서 스트라이드를 크게 하여 차원을 감소시킵니다.
- 하나 이상의 층을 제거합니다.
- 32비트 부동소수 대신 16비트 부동소수를 사용합니다.
- 여러 장치에 CNN을 분산합니다.

4. 같은 크기의 스트라이드의 합성곱 층 대신 최대 풀링층을 추가하는 이유는 무엇인가요?
 : 최대 풀링층은 파라미터를 전혀 가지고 있지 않지만 합성곱층은 상당한 양의 파라미터를
가지고 있기 때문입니다.

5. LRN 층을 추가해야 할 때는 언제인가요?
 : LRN 층은 가장 강하게 활성화되는 뉴런(픽셀)이 이웃한 특성 맵의 동일한 위치에 있는 뉴런(픽셀)을
억제하므로 특성 맵마다 특별하고 구분되게 만들어 넓은 범위의 특성을 탐색하도록 강제
합니다. 상위 층에서 필요한 저수준 특성을 많이 찾기 위해 일반적으로 하위층에서 사용됩니다.

6. LeNet-5와 비교해서 AlexNet의 혁신점은 무엇인가요? GoogLeNet, ResNet, SENet,
Xception의 혁신점은 무엇인가요?
 : LeNet-5와 비교했을 때 AlexNet의 주요 혁신은 (1) 더 크고 싶으며, (2) 합성곱층 위에 풀링층을
두지 않고 합성곱층으로만 직접 쌓아올렸다는 것입니다. GoogLeNet의 주요 혁신은 더 적은
파라미터로 종전의 CNN 구조보다 더 깊은 신경망을 만들 수 있도록 한 인셉션 모듈을 고안한
것입니다. ResNet의 주요 혁신은 100개 층 이상의 신경망을 구성할 수 있도록 만든 스킵 연결입니다.
이 구조의 단순함과 일광성도 큰 혁신입니다. SENet의 주요 혁신은 인셉션 네트워크에 있는 인셉션
모듈 또는 ResNet에 있는 잔차 유닛 다음에 SE블록(밀집 층 두 대골 구성된 네트워크)을 사용하여
특성 맵의 상대적 중요도를 보정한 것입니다. 마지막으로 Xception의 주요 혁신은 공간 패턴과
깊이별 패턴을 나누어보는 깊이별 분리 합성곱을 사용한 것입니다.

7. 완전 합성곱 신경망이 무엇인가요? 밀집 층을 어떻게 합성곱 층으로 바꿀 수 있나요?
 : 완전 합성곱 신경망은 합성곱과 풀링 층으로만 구성된 신경망입니다. FCN은 (적어도 최소 크기
이상이라면) 어떤 크기의 너비와 높이를 가진 이미지라도 효율적으로 처리할 수 있습니다.
이미지를 딱 한 번만 보기 때문에 객체 탐지와 시맨틱 분할에 유용합니다(반면 CNN은 이미지의
각 부위에서 여러 번 실행되어야 합니다). 몇 개의 밀집 층이 위에 놓인 CNN이 있다면 밀집 층을
합성곱 층으로 바꾸어 FCN을 만들 수 있습니다. 이 합성곱 층의 커널 크기는 가장 아래쪽 밀집
층의 입력 크기와 같고, 필터 개수는 밀집 층의 뉴런 개수와 같고 "valid"패딩을 사용합니다.
일반적으로 스트라이드는 1이지만 필요하면 더 큰 값을 지정할 수 있습니다. 활성화 함수는
밀집 층과 같은 활성화 함수를 사용합니다. 다른 밀집 층도 같은 방식으로 변환하지만 1x1
필터를 사용합니다. 실제로 훈련된 CNN을 밀집 층의 가중치 행렬의 크기를 적절히 바꾸어
이렇게 변경할 수 있습니다.

8. 시멘틱 분할에서 주요한 기술적 어려움은 무엇인가요?
 : CNN에서 신호가 층을 거쳐 전달되면 공간상의 정보가 많이 사라진다는 것입니다. 특히
풀링 층과 스트라이드가 1보다 큰 층입니다. 이런 공간상의 정보는 각 픽셀의 클래스를
정확히 예측하기 위해 복원하는 데 필요합니다.
