8장 연습문제

1. 데이터셋의 차원을 축소하는 주요 목적은무엇인가요? 대표적인 단점은 무엇인가요?
 : 목적 
- 훈련 알고리즘의 속도를 높이기 위해
- 데이터를 시각화하고 가장 중요한 특성에 대한 통찰을 얻기 위해
- 메모리 공간을 절약하기 위해(압축)
단점
- 일부 정보를 잃어버려 훈련 알고리즘의 성능을 감소시킬 수 있다.
- 계산 비용이 높다.
- 머신러닝 파이프라인의 복잡도를 증가시킨다.
- 변환된 데이터를 이해하기 어려운 경우가 많다.

2. 차원의 저주란 무엇인가요?
 : 저차원 공간에는 없는 많은 문제가 고차원 공간에서 일어난다는 사실을 뜻합니다.
머신러닝에서 무작위로 선택한 고차원 벡터는 매우 희소해서 과대적합의 위험이 크고,
많은 양의 데이터가 있지 않으면 데이터에 있는 패턴을 잡아내기 매우 어려운 것이 흔하다.

3. 데이터셋의 차원을 축소시키고 나서 이 작업을 원복할 수 있나요?
할 수 있다면 어떻게 가능할까요? 가능하지 않다면 왜일까요?
 : 여기에서 설명한 알고리즘 중 하나를 사용해 데이터셋의 차원이 축소되면 일부 정보가
차원 축소 과정에서 사라지기 때문에 이를 완벽하게 되돌리는 것은 불가능합니다.
(PCA 같은) 일부 알고리즘은 비교적 원본과 비슷한 데이터셋을 재구성할 수 있는 간단한 역변환
방법을 가지고 있지만, (T-SNE 같은) 다른 알고리즘들은 그렇지 않다.

4. 매우 비선형적인 데이터셋을 차원을 축소하는 데 PCA를 사용할 수 있을까요?
 : PCA는 불필요한 차원을 제거할 수 있기 때문에 매우 비선형적이더라도 대부분의 데이터셋에서
차원을 축소하는 데 사용할 수 있습니다. 그러나 불필요한 차원이 없다면 PCA의 차원 축소는 너무
많은 정보를 잃게 만듭니다. 즉, 스위스롤은 펼쳐야하며 말려진 것을 뭉개면 안 됩니다.

5. 설명된 분산을 95%로 지정한 PCA를 1000개의 차원을 가진 데이터셋에 적용한다고 가정하겠습니다.
결과 데이터셋의 차원은 얼마나 될까요?
 : 데이터셋에 따라 다르다.

6. 기본 PCA, 점진적 PCA, 랜덤 PCA, 커널 PCA는 어느 경우에 사용될까요?
 : 기본 PCA가 우선적으로 사용되지만 데이터셋 크기가 메모리에 맞을 때에 가능합니다.
점진적 PCA는 메모리에 담을 수 없는 대용량 데이터셋에 적합합니다. 하지만 기본 PCA보다
느리므로 데이터셋이 메모리 크기에 맞으면 기본 PCA를 사용해야 합니다. 점진적 PCA는
새로운 샘플이 발생될 때마다 실시간으로 PCA를 적용해야 하는 온라인 작업에서 사용 가능합니다.
랜덤 PCA는 데이터셋이 메모리 크기에 맞고 차원을 크게 축소시킬 때 사용됩니다.
이 경우에는 기본 PCA보다 훨씬 빠릅니다. 커널 PCA는 비선형 데이터셋에 유용합니다.

7. 어떤 데이터셋에 적용한 차원 축소 알고리즘의 성능을 어떻게 평가할 수 있을까요?
 : 직관적으로 데이터셋에서 너무 많은 정보를 잃지 않고 차원을 많이 제거할 수 있다면
차원 축소 알고리즘이 잘 작동한 것입니다. 이를 측정하는 한 가지 방법은 역변환을 수행해서
재구성 오차를 측정하는 것입니다. 하지만 모든 차원 축소 알고리즘이 역변환을 제공하지는
않습니다. 만약 차원 축소를 다른 머신러닝 알고리즘(ex. 랜덤 포레스트 분류기)을 적용하기 전에
전처리 단계로 상요한다면 두 번째 알고리즘의 성능을 측정해볼 수 있습니다. 즉, 차원 축소가
너무 많은 정보를 잃지 않았다면 원본 데이터셋을 사용했을 때와 비슷한 성능이 나와야 합니다.

8. 두 개의 차원 축소알고리즘을 연결할 수 있을까요?
 : 할 수 있다. PCA로 불필요한 차원을 대폭 제거하고 난 다음 LLE 같이 훨씬 느린 알고리즘을
적용하는 것이 대표적인 사례입니다. 이런 2단계 방식은 LLE만 사용했을 때와 거의 비슷한
성능을 내지만 속도가 몇 분의 1로 줄어들 것입니다.